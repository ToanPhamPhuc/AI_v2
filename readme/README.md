# Flappy Bird AI - Complete Documentation

This project implements different AI approaches to play Flappy Bird, from traditional Q-Learning to modern Deep Q-Learning (DQN).

## ğŸ—ï¸ Project Structure

```
AI_v2/
â”œâ”€â”€ ai/                    # AI implementations
â”‚   â”œâ”€â”€ ai_agent.py       # Q-Table based AI
â”‚   â”œâ”€â”€ dqn_agent.py      # Deep Q-Learning AI
â”‚   â””â”€â”€ training_loop.py  # Training utilities
â”œâ”€â”€ config/               # Configuration files
â”‚   â””â”€â”€ config.py         # Game and AI parameters
â”œâ”€â”€ data/                 # Data storage (organized)
â”‚   â”œâ”€â”€ scores/           # High score files
â”‚   â”œâ”€â”€ jsons/            # Q-tables and JSON data
â”‚   â””â”€â”€ pth/              # PyTorch model files
â”œâ”€â”€ game/                 # Game engine
â”‚   â”œâ”€â”€ main.py           # Main game logic
â”‚   â””â”€â”€ reward_system.py  # Reward calculation
â”œâ”€â”€ readme/               # Documentation
â”‚   â”œâ”€â”€ README.md         # Main documentation (this file)
â”‚   â””â”€â”€ README_DQN.md     # DQN-specific documentation
â”œâ”€â”€ tests/                # Test scripts
â”‚   â”œâ”€â”€ test_ai.py        # Test Q-table AI
â”‚   â””â”€â”€ test_dqn.py       # Test DQN AI
â”œâ”€â”€ trains/               # Training scripts (organized)
â”‚   â”œâ”€â”€ qLearning/        # Q-Table training scripts
â”‚   â”‚   â”œâ”€â”€ continuous_train.py
â”‚   â”‚   â”œâ”€â”€ multi_ai_train.py
â”‚   â”‚   â””â”€â”€ train_ai.py
â”‚   â””â”€â”€ dQLearning/       # DQN training scripts
â”‚       â””â”€â”€ dqn_train.py
â”œâ”€â”€ flappy-bird/          # Original game assets
â”œâ”€â”€ requirements.txt      # Python dependencies
â””â”€â”€ README.md            # Root overview
```

## ğŸš€ Quick Start

### Prerequisites
```bash
pip install -r requirements.txt
```

### Training Options

#### 1. Q-Table AI (Traditional)
```bash
python trains/qLearning/continuous_train.py
```
- Uses discrete state space
- Fast initial learning
- Good for understanding RL basics

#### 2. Deep Q-Learning (DQN)
```bash
python trains/dQLearning/dqn_train.py
```
- Uses neural network
- Continuous state space
- Better generalization
- More advanced approach

#### 3. Multi-AI Training
```bash
python trains/qLearning/multi_ai_train.py
```
- Trains multiple AIs simultaneously
- Faster strategy discovery
- Knowledge sharing between AIs

### Testing Trained AIs

#### Test Q-Table AI
```bash
python tests/test_ai.py
```

#### Test DQN AI
```bash
python tests/test_dqn.py
```

## ğŸ® Controls

During training:
- **Q**: Quit training
- **A**: Toggle coordinate axes
- **H**: Toggle hitboxes
- **C**: Toggle collision zones
- **B**: Clear pipe heatmap
- **G**: Toggle gap distance display

## ğŸ“Š Performance Comparison

| Approach | State Space | Memory Usage | Learning Speed | Generalization |
|----------|-------------|--------------|----------------|----------------|
| Q-Table | Discrete | High | Fast initial | Poor |
| DQN | Continuous | Low | Consistent | Excellent |
| Multi-AI | Discrete | High | Very fast | Good |

## ğŸ”§ Configuration

All parameters are configurable in `config/config.py`:

- **Game settings**: Screen size, physics, pipe gap
- **AI parameters**: Learning rates, exploration, rewards
- **DQN settings**: Network architecture, batch size, memory
- **File paths**: Organized data storage locations

## ğŸ“ Data Organization

- **Scores**: `data/scores/` - High score tracking
- **Models**: `data/pth/` - PyTorch DQN models
- **Q-Tables**: `data/jsons/` - Q-table and JSON data
- **Tests**: `tests/` - AI testing scripts

## ğŸ§ª Testing

Each AI approach has dedicated test scripts that:
- Load trained models
- Run without exploration (pure exploitation)
- Display real-time Q-values and statistics
- Show AI decision-making process

## ğŸ“ˆ Expected Results

With proper training:
- **Q-Table AI**: 20-50 pipes consistently
- **DQN AI**: 50+ pipes regularly
- **Multi-AI**: 30-60 pipes with faster learning

## ğŸ” Advanced Features

- **Pipe Heatmap**: Visual collision tracking
- **Adaptive Gap**: Dynamic difficulty adjustment
- **Experience Replay**: DQN memory buffer
- **Target Network**: Stable DQN training
- **Real-time Visualization**: Training progress display

## ğŸ¤ Contributing

Feel free to experiment with:
- Different network architectures
- Alternative reward functions
- Novel training strategies
- Advanced RL algorithms

The modular design makes it easy to extend and improve!

## ğŸ“š Documentation

- [DQN Implementation Details](README_DQN.md)
- [Configuration Guide](../config/config.py)
- [Training Scripts](../trains/)

## ğŸ¯ Next Steps

1. **Start with Q-Table**: Understand basic RL concepts
2. **Try DQN**: Experience modern deep RL
3. **Experiment**: Tune parameters and try new approaches
4. **Extend**: Add new features or algorithms

Happy training! ğŸ¦ğŸš€ 